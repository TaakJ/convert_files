@staticmethod
    def generate_excel_dataframe(full_path):
        data_list = {}
        sheet_list =  [sheet for sheet in pd.ExcelFile(full_path).sheet_names if sheet != 'StyleSheet']
        
        for name in sheet_list:
            df = pd.read_excel(full_path, sheet_name=name)
            df.columns = [value  if 'Unnamed' in col else col for col, value in df.iloc[0].items()]
            
            if set(df.columns.values) == set(df.iloc[0].values):
                df = df.drop(index=0, axis=0).reset_index(drop=True)
                
            df_new = df.to_dict('records')
            data_list[name] = df_new
            
            logging.info(f"Read Sheetname: '{name}' Status: 'Succees'")
        return data_list
        
    @staticmethod
    def generate_text_dataframe(full_path):
        data_list = {}
        
        files = open(full_path, 'rb')
        encoded = chardet.detect(files.read())['encoding']
        files.seek(0)
        decoded_data = files.read().decode(encoded)
        name =  str(Path(full_path).stem).upper()
        
        clean_lines_column = []
        clean_lines_value = []
        for line_num, line in enumerate(StringIO(decoded_data)):
            regex = re.compile(r'\w+.*')
            line_list = regex.findall(line)
            
            
            if line_list != []:
                gen_regex = re.sub(r'\W\s+','||',"".join(line_list).strip()).split('||')
                
                ## LDS-P_USERDETAIL ##
                if name == 'LDS-P_USERDETAIL':
                    if line_num == 0:
                        for col in gen_regex:
                            clean_lines_column = "".join(col).split(' ')
                    else:
                        nested_lines = [] 
                        for n, val in enumerate(gen_regex):
                            # Fix Column Rownum/UserID
                            if n == 0:
                                val = "".join(val).split(' ')
                                nested_lines.extend(val)
                            else:
                                nested_lines.append(val)
                        clean_lines_value.append(nested_lines)
                        
                ## DOCIMAGE ##     
                elif name == 'DOCIMAGE':
                    if line_num == 5:
                        nested_lines = []
                        for n_col, col in enumerate(gen_regex):
                            if n_col == 4:
                                col = "".join(col).split(' ')
                                clean_lines_column.extend(col)
                            else:
                                clean_lines_column.append(col)
                    elif line_num > 5:
                        nested_lines = []
                        for n, val in enumerate(gen_regex):
                            if n == 3:
                                # Fix Column STAMP/ADD_ID
                                val = "".join(val).split(' ')
                                nested_lines.extend(val)
                            else:
                                nested_lines.append(val)
                        clean_lines_value.append(nested_lines)
                        
                ## ADM ##     
                elif name == 'ADM':
                    nested_lines = []
                    for val in gen_regex:
                        nested_lines.append(val)
                    clean_lines_value.append(nested_lines)
        
        df = pd.DataFrame(clean_lines_value)
        if clean_lines_column != []:
            df.columns = clean_lines_column
            
        df_new = df.to_dict('records')
        data_list[name] = df_new
        
        logging.info(f"Read Sheetname: '{name}' Status: 'Succees'")
        return data_list




def compare_data_to_file(self):
        
        logging.info('Compare Data to Files')
        csv_name = f"{FOLDER.LOG}DD_{self.date.strftime('%d%m%Y')}.csv"
        
        try:
            for _dict in self.fn_log:
                if _dict['source'] == 'write':
                    ## new data record
                    df_new = pd.DataFrame( _dict['data'])
                    
                    if glob.glob(csv_name, recursive=True):
                        ## old data record
                        df_csv = pd.read_csv(csv_name)
                        
                        ## compare data
                    #     if len(df_csv.index) > len(df_new.index):
                    #         skip_idx = [idx for idx in list(df_csv.index) if idx not in list(df_new.index)]
                    #         compare_df = df_csv.copy().astype('object')
                    #         diff_df = df_new.copy().astype('object')
                    #         state = 'skip_rows'
                    #     else:
                    #         skip_idx = []
                    #         compare_df = df_new.copy().astype('object')
                    #         diff_df = df_csv.copy().astype('object')
                    #         state = 'update_rows'
                        
                    #     for row_idx in list(compare_df.index):
                    #         if row_idx not in skip_idx:
                    #             cnt_change = []
                    #             for (_, val), (col_, val_) in zip(compare_df.items(), diff_df.items()):
                    #                 if row_idx in list(val_.index):
                    #                     ## check not change record
                    #                     if val.iloc[row_idx] == val_.iloc[row_idx]:
                    #                         compare_df.at[row_idx, col_] = val_.iloc[row_idx]
                    #                         compare_df.at[row_idx, 'change'] = 0
                    #                     else:
                    #                         ## check change record
                    #                         cnt_change += ['change']
                    #                         if len(cnt_change) == 1 and col_ == 'LastUpdatedDate':
                    #                             if state == 'update_rows':
                    #                                 compare_df.at[row_idx, col_] = val_.iloc[row_idx]
                    #                             else:
                    #                                 compare_df.at[row_idx, col_] = val.iloc[row_idx]
                    #                         else:
                    #                             if state == 'skip_rows':
                    #                                 compare_df.at[row_idx, col_] = val_.iloc[row_idx]
                    #                             else:
                    #                                 compare_df.at[row_idx, col_] = val.iloc[row_idx]
                    #                         compare_df.at[row_idx, 'change'] = len(cnt_change)
                    #                 else:
                    #                     ## check new record
                    #                     compare_df.at[row_idx, col_] = val.iloc[row_idx]
                    #                     compare_df.at[row_idx, 'change'] = 14
                    #         else:
                    #             ## check delete record
                    #             compare_df.iloc[row_idx] = 'drop_rows'
                    #             compare_df.at[row_idx, 'change'] = 14
                    #     compare_df = compare_df[compare_df.iloc[:,14] > 1].iloc[:, :-1]
                        
                    #     # read from file
                    #     with open(csv_name, 'r') as reader:
                    #         csvin = csv.DictReader(reader, skipinitialspace=True)
                    #         to_update = {idx: rows for idx, rows in compare_df.to_dict('index').items()}
                    #         from_update = {idx: rows for idx, rows in enumerate(csvin)}
                            
                    #         ## replace data in files
                    #         for row_idx in to_update:
                    #             if row_idx in from_update:
                    #                 ## update record in files
                    #                 from_update[row_idx].update(to_update[row_idx])
                    #                 logging.info(f"Update record num: {row_idx}, data: {from_update[row_idx]}")
                    #             else:
                    #                 ## insert record in files
                    #                 from_update.update({row_idx: to_update[row_idx]})
                    #                 logging.info(f"Insert record num: {row_idx}, data: {from_update[row_idx]}")
                        
                    #     ## write to file
                    #     with open(csv_name, 'w', newline='') as writer:
                    #         csvout = csv.DictWriter(writer, csvin.fieldnames)
                    #         csvout.writeheader()
                    #         for row_idx in from_update:
                    #             if row_idx not in skip_idx:
                    #                 csvout.writerow(from_update[row_idx])
                    # else:
                    #     df_new.to_csv(csv_name, index=False, header=True)
                    
        except Exception as err:
            print(f"err: {err}")




            # for c_idx, n_idx in zip_longest(list(csv_df.index), list(new_df.index)):
        #     if c_idx not in cls.skip_rows:
        #         i = 0
        #         cnt_change = []
        #         for csv, new in zip_longest(csv_df.items(), new_df.items()):
        #             if c_idx == n_idx:
        #                 if csv[1].iloc[n_idx] == new[1].iloc[n_idx]:
        #                     ## not change record
        #                     df.at[n_idx, csv[0]] = csv[1].iloc[n_idx]
        #                 else:
        #                     i += 1
        #                     cnt_change += [{n_idx:i}]
        #                     ## change only column LastUpdatedDate == not change record
        #                     if len(cnt_change) == 1 and csv[0] == 'LastUpdatedDate':
        #                         df.at[n_idx, csv[0]] = csv[1].iloc[n_idx]
        #                     else:
        #                         ## change record
        #                         df.at[n_idx, csv[0]] = new[1].iloc[n_idx]
        #                     df.at[n_idx, 'change'] = len(cnt_change)
                                
        #             elif c_idx is None and n_idx is not None:
        #                 ## insert record
        #                 df.at[n_idx, csv[0]] = new[1].iloc[n_idx]
        #                 df.at[n_idx, 'change'] = 14
        #     else:
        #         ## skip record
        #         df.loc[c_idx] = 'skip_rows'
        #         df.at[c_idx, 'change'] = 14
        
        # df = df.loc[df['change'] > 1].drop(['change'], axis=1)



         # @classmethod
    # def update_data_tmp(cls, tmp_df, new_df):
        
    #     if len(tmp_df.index) > len(new_df.index):
    #         cls.skip_rows = [idx for idx in list(tmp_df.index) if idx not in list(new_df.index)]
        
    #     union_index = np.union1d(tmp_df.index, new_df.index)
    #     ## set old record
    #     tmp_df = tmp_df.reindex(index=union_index, columns=tmp_df.columns)
        
    #     ## set new record
    #     new_df = new_df.reindex(index=union_index, columns=new_df.columns)
        
    #     ## set column change / skip in new_df 
    #     new_df['change'] = pd.DataFrame(np.where(new_df.ne(tmp_df), True, False), index=new_df.index, columns=new_df.columns)\
    #         .apply(lambda data: data.value_counts()[True], axis=1)
    #     new_df['skip'] = new_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     ## set column change / skip in tmp_df 
    #     tmp_df['change'] = new_df['change']
    #     tmp_df['skip'] = tmp_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     for idx in union_index:
    #         if idx not in cls.skip_rows:
    #             for tmp, new in zip(tmp_df.items(), new_df.items()):
    #                 if tmp_df.loc[idx, 'skip'] == new_df.loc[idx, 'skip']:
    #                     if new_df.loc[idx, 'change'] <= 1:
    #                         ## not change rows
    #                         tmp_df.at[idx, tmp[0]] = tmp[1].iloc[idx]
    #                     else:
    #                         ## update rows
    #                         tmp_df.at[idx, tmp[0]] = new[1].iloc[idx]
    #                 else:
    #                     ## insert rows
    #                     tmp_df.at[idx, tmp[0]] = new[1].iloc[idx]
    #         else:
    #             ## delete rows
    #             continue
            
    #     tmp_df = tmp_df.loc[tmp_df['change'] > 1].drop(['change', 'skip'], axis=1)
    #     to_tmp = {idx: rows for idx, rows in tmp_df.to_dict('index').items()}
        
    #     return to_tmp


     # read file excel daily
        # workbook = xlrd.open_workbook(target_name)
        # sheet = workbook.sheet_by_index(0)
        # rows = sheet.get_rows()
        
        # list_target =[]
        # for row in rows:
        #     if all([cell.ctype in (xlrd.XL_CELL_EMPTY, xlrd.XL_CELL_BLANK) for cell in row]):
        #         break
        #     else:
        #         list_target += [[cell.value for cell in row]] 
                
        # target_df = pd.DataFrame(list_target)
        # target_df.columns = target_df.iloc[0].values
        # target_df = target_df[1:]
        # target_df = target_df.reset_index(drop=True)