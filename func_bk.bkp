@staticmethod
    def generate_excel_dataframe(full_path):
        data_list = {}
        sheet_list =  [sheet for sheet in pd.ExcelFile(full_path).sheet_names if sheet != 'StyleSheet']
        
        for name in sheet_list:
            df = pd.read_excel(full_path, sheet_name=name)
            df.columns = [value  if 'Unnamed' in col else col for col, value in df.iloc[0].items()]
            
            if set(df.columns.values) == set(df.iloc[0].values):
                df = df.drop(index=0, axis=0).reset_index(drop=True)
                
            df_new = df.to_dict('records')
            data_list[name] = df_new
            
            logging.info(f"Read Sheetname: '{name}' Status: 'Succees'")
        return data_list
        
    @staticmethod
    def generate_text_dataframe(full_path):
        data_list = {}
        
        files = open(full_path, 'rb')
        encoded = chardet.detect(files.read())['encoding']
        files.seek(0)
        decoded_data = files.read().decode(encoded)
        name =  str(Path(full_path).stem).upper()
        
        clean_lines_column = []
        clean_lines_value = []
        for line_num, line in enumerate(StringIO(decoded_data)):
            regex = re.compile(r'\w+.*')
            line_list = regex.findall(line)
            
            
            if line_list != []:
                gen_regex = re.sub(r'\W\s+','||',"".join(line_list).strip()).split('||')
                
                ## LDS-P_USERDETAIL ##
                if name == 'LDS-P_USERDETAIL':
                    if line_num == 0:
                        for col in gen_regex:
                            clean_lines_column = "".join(col).split(' ')
                    else:
                        nested_lines = [] 
                        for n, val in enumerate(gen_regex):
                            # Fix Column Rownum/UserID
                            if n == 0:
                                val = "".join(val).split(' ')
                                nested_lines.extend(val)
                            else:
                                nested_lines.append(val)
                        clean_lines_value.append(nested_lines)
                        
                ## DOCIMAGE ##     
                elif name == 'DOCIMAGE':
                    if line_num == 5:
                        nested_lines = []
                        for n_col, col in enumerate(gen_regex):
                            if n_col == 4:
                                col = "".join(col).split(' ')
                                clean_lines_column.extend(col)
                            else:
                                clean_lines_column.append(col)
                    elif line_num > 5:
                        nested_lines = []
                        for n, val in enumerate(gen_regex):
                            if n == 3:
                                # Fix Column STAMP/ADD_ID
                                val = "".join(val).split(' ')
                                nested_lines.extend(val)
                            else:
                                nested_lines.append(val)
                        clean_lines_value.append(nested_lines)
                        
                ## ADM ##     
                elif name == 'ADM':
                    nested_lines = []
                    for val in gen_regex:
                        nested_lines.append(val)
                    clean_lines_value.append(nested_lines)
        
        df = pd.DataFrame(clean_lines_value)
        if clean_lines_column != []:
            df.columns = clean_lines_column
            
        df_new = df.to_dict('records')
        data_list[name] = df_new
        
        logging.info(f"Read Sheetname: '{name}' Status: 'Succees'")
        return data_list



         # @classmethod
    # def update_data_tmp(cls, tmp_df, new_df):
        
    #     if len(tmp_df.index) > len(new_df.index):
    #         cls.skip_rows = [idx for idx in list(tmp_df.index) if idx not in list(new_df.index)]
        
    #     union_index = np.union1d(tmp_df.index, new_df.index)
    #     ## set old record
    #     tmp_df = tmp_df.reindex(index=union_index, columns=tmp_df.columns)
        
    #     ## set new record
    #     new_df = new_df.reindex(index=union_index, columns=new_df.columns)
        
    #     ## set column change / skip in new_df 
    #     new_df['change'] = pd.DataFrame(np.where(new_df.ne(tmp_df), True, False), index=new_df.index, columns=new_df.columns)\
    #         .apply(lambda data: data.value_counts()[True], axis=1)
    #     new_df['skip'] = new_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     ## set column change / skip in tmp_df 
    #     tmp_df['change'] = new_df['change']
    #     tmp_df['skip'] = tmp_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     for idx in union_index:
    #         if idx not in cls.skip_rows:
    #             for tmp, new in zip(tmp_df.items(), new_df.items()):
    #                 if tmp_df.loc[idx, 'skip'] == new_df.loc[idx, 'skip']:
    #                     if new_df.loc[idx, 'change'] <= 1:
    #                         ## not change rows
    #                         tmp_df.at[idx, tmp[0]] = tmp[1].iloc[idx]
    #                     else:
    #                         ## update rows
    #                         tmp_df.at[idx, tmp[0]] = new[1].iloc[idx]
    #                 else:
    #                     ## insert rows
    #                     tmp_df.at[idx, tmp[0]] = new[1].iloc[idx]
    #         else:
    #             ## delete rows
    #             continue
            
    #     tmp_df = tmp_df.loc[tmp_df['change'] > 1].drop(['change', 'skip'], axis=1)
    #     to_tmp = {idx: rows for idx, rows in tmp_df.to_dict('index').items()}
        
    #     return to_tmp


     # read file excel daily
        # workbook = xlrd.open_workbook(target_name)
        # sheet = workbook.sheet_by_index(0)
        # rows = sheet.get_rows()
        
        # list_target =[]
        # for row in rows:
        #     if all([cell.ctype in (xlrd.XL_CELL_EMPTY, xlrd.XL_CELL_BLANK) for cell in row]):
        #         break
        #     else:
        #         list_target += [[cell.value for cell in row]] 
                
        # target_df = pd.DataFrame(list_target)
        # target_df.columns = target_df.iloc[0].values
        # target_df = target_df[1:]
        # target_df = target_df.reset_index(drop=True)


			 # def mapping_data(call_func):
    #     def fn_data_mapping(self):
    #         for _dict in call_func(self):
    #             try:
    #                 for sheets, data in _dict['data'].items():
    #                     logging.info(f"Mapping Data Sheet: '{sheets}'")
    #                     ## ignore UserWarning: Data Validation no header in sheet: USER REPORT
    #                     warnings.simplefilter(action='ignore', category=UserWarning)
    #                     df = pd.DataFrame(data)
    #                     df.columns = df.iloc[0].values
    #                     df = df[1:]
    #                     df = df.reset_index(drop=True)
    #                     _dict.update({'data': df.to_dict('list')})
                        
    #             except Exception as err:
    #                 _dict.update({'errors': str(err)})
                    
    #         if 'errors' in self.fn_log[0]:
    #             raise CustomException(self.fn_log)
                
    #         return self.fn_log
        
    #     return fn_data_mapping


    # def setup_project():
#     parser = argparse.ArgumentParser()
#     parser.add_argument("-s","-RAW",
#                         required=False, 
#                         type=str
#                         )
#     parser.add_argument("-x","--EXPORT",
#                         required=False, 
#                         type=str
#                         )
#     parser.add_argument("-o","--output",
#                         required=False, 
#                         choices=[1,2],
#                         type=int,
#                         default=1,
#                         help = (' 0 = Excel file , '
#                                 ' 1 = CSV file',
#                                 ' 2 = text file ')
#                         )
#     method_args = parser.parse_args()

 # self.__log = list({key['source']: key for key in log}.values())


  # def compare_data_to_file(self):
        
    #     logging.info('Compare Data to Tmp files..')
    #     csv_name = f"{self.LOG}DD_{self.date.strftime('%d%m%Y')}.csv"
    #     status = 'failed'
        
    #     for key in self.logging:
    #         try:
    #             if key['source'] == 'Target_file':
    #                 new_df = pd.DataFrame( key['data'])
    #                 key.update({'full_path': csv_name, 'status': status})
                    
    #                 if glob.glob(csv_name, recursive=True):
    #                     tmp_df = pd.read_csv(csv_name)
    #                     output = self.check_up_data(tmp_df, new_df)
                        
    #                     ## read from file
    #                     start_rows = 2 
    #                     with open(csv_name, 'r') as reader:
    #                         csvin = csv.DictReader(reader, skipinitialspace=True)
    #                         rows = {idx: rows for idx, rows in enumerate(csvin)}
    #                         if output != {}:
    #                             for idx in output:
    #                                 if idx in rows:
    #                                     if idx not in self.skip_rows:
                                            
    #                                         change_value = {}
    #                                         for value in rows[idx]: 
    #                                             if value in output[idx] and (str(rows[idx][value]) != str(output[idx][value])):
    #                                                 change_value.update({value: f"{rows[idx][value]} => {output[idx][value]}"})
    #                                                 rows[idx].update({value: output[idx][value]})
    #                                         logging.info(f"\033[1mUpdated Rows: {idx + start_rows} in Tmp files. Recorded: {change_value}\033[0m")
                                            
    #                                     else:
    #                                         logging.info(f"\033[1mDeleted Rows: {idx + start_rows} in Tmp files.\033[0m")
    #                                         continue
    #                                 else:
    #                                     rows.update({idx: output[idx]})
    #                                     logging.info(f"\033[1mInserted Rows: {idx + start_rows} in Tmp files. Recorded: {rows[idx]}\033[0m")
    #                         else:
    #                             logging.info("\033[1mNo changes data in Tmp files.\033[0m")
                                
    #                     ## write to file
    #                     with open(csv_name, 'w', newline='') as writer:
    #                         csvout = csv.DictWriter(writer, csvin.fieldnames)
    #                         csvout.writeheader()
    #                         for idx in rows:
    #                             if idx not in self.skip_rows:
    #                                 csvout.writerow(rows[idx])
    #                     writer.closed
    #                     status = 'successed'
                        
    #                 else:
    #                     new_df.to_csv(csv_name, index=False, header=True)    
    #                     status = 'successed'
    #                     logging.info(f"\033[1mCreate Tmp files '{csv_name}.'\033[0m")
                        
    #                 key.update({'status': status})
    #                 logging.info(f"Write to Tmp files status: {status}.")
                    
    #         except Exception as err:
    #             key.update({'errors': err})
            
    #         if 'errors' in key:
    #             raise CustomException(self.logging)


     # @classmethod
    # def check_up_data(cls, mark_df, new_df):
    #     logging.info("Check Changed Data or Not Change in Data..")
        
    #     if len(mark_df.index) > len(new_df.index):
    #         cls.skip_rows = [idx for idx in list(mark_df.index) if idx not in list(new_df.index)]
        
    #     union_index = np.union1d(mark_df.index, new_df.index)
    #     ## set old record
    #     mark_df = mark_df.reindex(index=union_index, columns=mark_df.columns)
        
    #     ## set new record
    #     new_df = new_df.reindex(index=union_index, columns=new_df.columns)
        
    #     ## set column change / skip in new_df 
    #     new_df['change'] = pd.DataFrame(np.where(new_df.ne(mark_df), True, False), index=new_df.index, columns=new_df.columns)\
    #         .apply(lambda x: (x==True).sum(), axis=1)
    #     new_df['skip'] = new_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     ## set column change / skip in old_df 
    #     mark_df['change'] = new_df['change']
    #     mark_df['skip'] = mark_df.apply(lambda x: x.isna()).all(axis=1)
        
    #     for idx in union_index:
    #         if idx not in cls.skip_rows:
    #             for mark, new in zip(mark_df.items(), new_df.items()):
    #                 if mark_df.loc[idx, 'skip'] == new_df.loc[idx, 'skip']:
    #                     if new_df.loc[idx, 'change'] <= 1:
    #                         ## not change rows
    #                         mark_df.at[idx, mark[0]] = mark[1].iloc[idx]
    #                     else:
    #                         ## update rows
    #                         mark_df.at[idx, mark[0]] = new[1].iloc[idx]
    #                 else:
    #                     ## insert rows
    #                     mark_df.at[idx, mark[0]] = new[1].iloc[idx]
    #         else:
    #             ## delete rows
    #             continue
            
    #     mark_df = mark_df.loc[mark_df['change'] > 1].drop(['change', 'skip'], axis=1)
    #     output = mark_df.to_dict('index')
        
    #     return output
    

    @classmethod
    # def get_data_target(cls, target_name, tmp_df):
        
    #     target_df = pd.read_excel(target_name)
    #     start_rows = 2
    #     output = {}
        
    #     if not target_df.empty:
    #         date = tmp_df['CreateDate'].unique()
            
    #         ## select row not use for compare
    #         mark_df = target_df[~target_df['CreateDate'].isin(date)].to_dict('index')
    #         max_rows = max(mark_df, default=0)
            
    #         ## select row use for compare / mark data for compare with tmp
    #         select_df = target_df[target_df['CreateDate'].isin(date)].reset_index(drop=True)
    #         compare_data = cls.check_up_data(select_df, tmp_df)
    #         select_date = select_df.to_dict('index')
            
    #         ## compare target change / not change
    #         for key, value in compare_data.items():
    #             if key not in cls.skip_rows:
    #                 try:
    #                     if value != select_date[key]:
    #                         select_date.pop(key)
    #                     select_date[key] = value
    #                 except KeyError:
    #                     select_date[key] = value
    #             else:
    #                 if value == select_date[key]:
    #                     select_date.pop(key)
                        
    #         for value in select_date.values():
    #             max_rows += 1
    #             mark_df[max_rows] = value
    #             mark_df[max_rows]['inserted'] =  True
            
    #         ## set ordered rows 
    #         ordered = sorted([mark_df[value] for value in mark_df], key=operator.itemgetter('CreateDate'))
    #         sorted_rows = iter(ordered)
    #         while True:
    #             try:
    #                 value = next(sorted_rows)
    #                 output.update({start_rows: value})
                    
    #                 if value.get('inserted'):
    #                     cls.insert_rows.append(start_rows)
    #                     value.pop('inserted')
                        
    #             except StopIteration:
    #                 break
    #             start_rows += 1
                
    #     else:
    #         tmp_df = tmp_df.to_dict('index')
    #         output = {start_rows + key: value for key,value in tmp_df.items()}
        
    #     return output


       # def write_to_file(self):
        
    #     logging.info("Write Data to Target files..")
    #     target_name = f"{self.EXPORT}Application Data Requirements.xlsx"
        
    #     workbook = openpyxl.load_workbook(target_name)
    #     get_sheet = workbook.get_sheet_names()
    #     sheet = workbook.get_sheet_by_name(get_sheet[0])
    #     workbook.active
        
    #     for key in self.logging:
    #         try:
    #             status = 'failed'
    #             if key['source'] == 'Target_file':
    #                 filename = key['full_path']
    #                 status = key['status']
                    
    #                 if status == 'successed':
    #                     tmp_df = pd.read_csv(filename)
    #                     output = self.get_data_target(target_name, tmp_df)
    #                 else:
    #                     raise CustomException(self.logging)
    #                 key.update({'full_path': target_name, 'status': status})
                    
    #                 ## write data to target file
    #                 start_rows = 2
    #                 while start_rows <= max(output):
    #                     if start_rows in self.insert_rows:
    #                         for idx, value in enumerate(output[start_rows].values(), 1):
    #                             sheet.cell(row=start_rows, column=idx).value = value
    #                         logging.info(f"\033[1mWrote Rows: {start_rows} in Target files. Recorded: {output[start_rows]}\033[0m")
                        
    #                     else:
    #                         for idx, value in enumerate(output[start_rows].values(), 1):
    #                             sheet.cell(row=start_rows, column=idx).value = value   
    #                     start_rows += 1
                    
    #                 ## check deleted rows
    #                 if len(self.skip_rows) != 0:
    #                     rows = max(output) + 1
    #                     logging.info(f"\033[1mDeleted Rows: {rows} to {sheet.max_row} tp  in Target files.\033[0m")
    #                     sheet.delete_rows(idx=rows, amount=len(self.skip_rows))
                    
    #                 workbook.save(target_name)
    #                 status = 'successed'
                    
    #                 key.update({'status': status})
    #                 logging.info(f"write to target files status: {status}.")
                    
    #         except Exception as err:
    #             key.update({'errors': err})
                
    #     if 'errors' in key:
    #         raise CustomException(self.logging)



    # diff_date = {start_rows + row: diff_date[row] for row in diff_date}


# from openpyxl.worksheet.dimensions import ColumnDimension, DimensionHolder
# from openpyxl.utils import get_column_letter